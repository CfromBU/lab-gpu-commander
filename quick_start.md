# Lab-GPU å¿«é€Ÿä¸Šæ‰‹æŒ‡å—

## ğŸš€ 5 åˆ†é’Ÿå¿«é€Ÿå¼€å§‹

### 1. å¯åŠ¨æœåŠ¡å¹¶æ·»åŠ èŠ‚ç‚¹

```bash
# æ¿€æ´»ä½ çš„ç¯å¢ƒ
conda activate graphAR

# å¯åŠ¨ master
lab-gpu server start --role master --host 127.0.0.1

# æ·»åŠ  GPU èŠ‚ç‚¹ï¼ˆæ ¹æ®ä½ çš„å®é™…é…ç½®ä¿®æ”¹ï¼‰
lab-gpu server add-node --name node-1 --gpus 2 --vram 24 --gpu-type "RTX 3090"
```

### 2. æäº¤ä»»åŠ¡ï¼ˆä¸‰é€‰ä¸€ï¼‰

#### é€‰é¡¹ Aï¼šå•ä¸ªä»»åŠ¡
```bash
lab-gpu submit --mem 10G --priority normal "python train.py"
```

#### é€‰é¡¹ Bï¼šä½¿ç”¨ä½ çš„ tasks.json
```bash
lab-gpu submit-batch --file tasks.json
```

#### é€‰é¡¹ Cï¼šå…ˆæµ‹è¯•å†æäº¤
```bash
# å…ˆçœ‹çœ‹èƒ½å¦åˆ†é…ï¼ˆä¸å®é™…æäº¤ï¼‰
lab-gpu submit-batch --file tasks.json --dry-run

# ç¡®è®¤åå†æäº¤
lab-gpu submit-batch --file tasks.json
```

### 3. æ‰§è¡Œè°ƒåº¦å¹¶æŸ¥çœ‹çŠ¶æ€

```bash
# è§¦å‘è°ƒåº¦
lab-gpu server tick

# æŸ¥çœ‹çŠ¶æ€
lab-gpu status

# æˆ–è€…ä½¿ç”¨ TUI å¯è§†åŒ–ç•Œé¢
lab-gpu tui
```

## ğŸ“‹ tasks.json æ ¼å¼è¯´æ˜

ä½ çš„ `tasks.json` å½“å‰æ ¼å¼ï¼š

```json
{
  "tasks": [
    {"cmd": "python train_a.py", "mem": "10G", "priority": "normal"},
    {"cmd": "python train_b.py", "min_vram_gb": 8, "priority": "high", "time_limit": 1200}
  ]
}
```

### æ”¯æŒçš„å­—æ®µï¼š

| å­—æ®µ | ç±»å‹ | è¯´æ˜ | ç¤ºä¾‹ |
|------|------|------|------|
| `cmd` | å­—ç¬¦ä¸² | **å¿…å¡«**ï¼Œè¦æ‰§è¡Œçš„å‘½ä»¤ | `"python train.py"` |
| `mem` | å­—ç¬¦ä¸² | æ˜¾å­˜éœ€æ±‚ï¼ˆå¸¦å•ä½ï¼‰ | `"10G"`, `"16G"` |
| `min_vram_gb` | æ•°å­— | æ˜¾å­˜éœ€æ±‚ï¼ˆGBï¼Œä¸ mem äºŒé€‰ä¸€ï¼‰ | `8`, `12` |
| `priority` | å­—ç¬¦ä¸² | ä¼˜å…ˆçº§ï¼šhigh/normal/low | `"high"` |
| `env` | å­—ç¬¦ä¸² | Conda ç¯å¢ƒå | `"graphAR"` |
| `gpu_type` | å­—ç¬¦ä¸² | æŒ‡å®š GPU å‹å· | `"RTX 3090"` |
| `time_limit` | æ•°å­— | æ—¶é—´é™åˆ¶ï¼ˆç§’ï¼‰ | `3600` |

### ç¤ºä¾‹é…ç½®ï¼š

```json
{
  "tasks": [
    {
      "cmd": "python train_resnet.py --epochs 100",
      "mem": "16G",
      "priority": "high",
      "env": "graphAR",
      "gpu_type": "RTX 3090"
    },
    {
      "cmd": "python train_bert.py",
      "min_vram_gb": 24,
      "priority": "normal",
      "time_limit": 7200
    },
    {
      "cmd": "python long_experiment.py",
      "mem": "12G",
      "priority": "low",
      "env": "pytorch"
    }
  ]
}
```

## ğŸ¯ å¸¸è§ä½¿ç”¨åœºæ™¯

### åœºæ™¯ 1ï¼šæäº¤ä¸€æ‰¹è®­ç»ƒä»»åŠ¡
```bash
# ç¼–è¾‘ tasks.jsonï¼Œæ·»åŠ ä½ çš„ä»»åŠ¡
# ç„¶åæ‰¹é‡æäº¤
lab-gpu submit-batch --file tasks.json
lab-gpu server tick
lab-gpu status
```

### åœºæ™¯ 2ï¼šæŸ¥çœ‹å’Œç®¡ç†ä»»åŠ¡
```bash
# ä½¿ç”¨ TUI ç•Œé¢ï¼ˆæ¨èï¼‰
lab-gpu tui

# æˆ–è€…å‘½ä»¤è¡Œ
lab-gpu status --json
lab-gpu logs 1 -f  # æŸ¥çœ‹ä»»åŠ¡ 1 çš„æ—¥å¿—
```

### åœºæ™¯ 3ï¼šä¼˜å…ˆæ‰§è¡Œé‡è¦ä»»åŠ¡
```bash
# æäº¤é«˜ä¼˜å…ˆçº§ä»»åŠ¡
lab-gpu submit --mem 16G --priority high "python urgent_exp.py"
lab-gpu server tick
```

### åœºæ™¯ 4ï¼šæµ‹è¯•ä»»åŠ¡é…ç½®
```bash
# å…ˆ dry-run çœ‹çœ‹èƒ½å¦åˆ†é…
lab-gpu submit --mem 32G --priority high --dry-run "python large_model.py"

# å¦‚æœè¿”å› placement: nullï¼Œè¯´æ˜æ˜¾å­˜ä¸è¶³ï¼Œéœ€è¦è°ƒæ•´
```

## ğŸ’¡ å®ç”¨æŠ€å·§

1. **ä¼˜å…ˆçº§ç­–ç•¥**
   - `high`ï¼šç´§æ€¥ä»»åŠ¡ï¼Œä¼˜å…ˆè°ƒåº¦
   - `normal`ï¼šæ™®é€šä»»åŠ¡
   - `low`ï¼šä¸æ€¥çš„ä»»åŠ¡ï¼Œä¼šåœ¨å¤œé—´è‡ªåŠ¨åŠ é€Ÿ

2. **æ—¶é—´é™åˆ¶**
   - è®¾ç½® `time_limit` å¯ä»¥è®©çŸ­ä»»åŠ¡é€šè¿‡å›å¡«ç­–ç•¥ä¼˜å…ˆè¿è¡Œ

3. **ç¯å¢ƒç®¡ç†**
   - ä½¿ç”¨ `env` å­—æ®µæŒ‡å®š conda ç¯å¢ƒ
   - æˆ–åœ¨å‘½ä»¤ä¸­ä½¿ç”¨ `conda run -n env_name python script.py`

4. **æ—¥å¿—æŸ¥çœ‹**
   - é»˜è®¤æ—¥å¿—ä½ç½®ï¼š`/nas/logs/{task_id}.log`
   - å¯ä»¥ç”¨ `--log-root` æ”¹å˜æ—¥å¿—ç›®å½•

## ğŸ”§ æ•…éšœæ’é™¤

### é—®é¢˜ï¼šä»»åŠ¡æäº¤åçŠ¶æ€ä¸€ç›´æ˜¯ Pending: 0
**è§£å†³**ï¼šéœ€è¦æ‰‹åŠ¨æ‰§è¡Œ `lab-gpu server tick` è§¦å‘è°ƒåº¦

### é—®é¢˜ï¼šä»»åŠ¡æ— æ³•åˆ†é… GPU
**è§£å†³**ï¼š
1. æ£€æŸ¥æ˜¯å¦æ·»åŠ äº† GPU èŠ‚ç‚¹ï¼š`lab-gpu status`
2. æ£€æŸ¥æ˜¾å­˜éœ€æ±‚æ˜¯å¦è¶…è¿‡èŠ‚ç‚¹å®¹é‡
3. ä½¿ç”¨ `--dry-run` æµ‹è¯•åˆ†é…

### é—®é¢˜ï¼šéœ€è¦ä¿®æ”¹æ—¥å¿—ç›®å½•
**è§£å†³**ï¼š
```bash
lab-gpu agent run --task-id 1 --mem-used 10 --log-root ./logs "python train.py"
```

## ğŸ“š æ›´å¤šä¿¡æ¯

è¯¦ç»†æ–‡æ¡£è¯·å‚è€ƒï¼š`README.md`
