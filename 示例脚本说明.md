# Lab-GPU 示例脚本使用说明

我为你创建了多个示例脚本，可以直接运行，无需预设 `tasks.json`。

## 📋 可用脚本

### 1️⃣ **完整版脚本** (推荐)

#### Bash 版本
```bash
bash run_examples_with_labgpu.sh
```

**功能：**
- ✅ 自动启动 Lab-GPU Master 服务
- ✅ 添加 GPU 节点
- ✅ 提交 6 个示例任务（包含不同优先级）
- ✅ 执行调度
- ✅ 显示任务状态
- ✅ 提供详细的执行说明

**适合：** 第一次使用，想了解完整流程

---

#### Python 版本
```bash
python run_examples.py
# 或
./run_examples.py
```

**功能：**
- ✅ 与 Bash 版本相同的功能
- ✅ 更好的错误处理
- ✅ 自动提取任务 ID
- ✅ 完成后可选择启动 TUI 界面

**适合：** 喜欢 Python 或需要扩展功能

---

### 2️⃣ **简化版脚本**

```bash
bash run_examples_simple.sh
```

**功能：**
- ✅ 快速启动服务
- ✅ 提交 3 个基础任务
- ✅ 执行调度并显示状态

**适合：** 快速测试

---

### 3️⃣ **带 TUI 监控脚本**

```bash
bash run_and_monitor.sh
```

**功能：**
- ✅ 提交任务后自动启动 TUI 可视化界面
- ✅ 可以实时查看任务状态
- ✅ 支持交互式管理（kill/retry/提升优先级）

**适合：** 需要实时监控任务

---

## 🎯 示例任务说明

所有脚本都使用 `examples/` 目录中的示例程序：

| 任务 | 命令 | 显存 | 优先级 | 说明 |
|------|------|------|--------|------|
| GPU 分配 | `gpu_alloc.py` | 2G | high | 申请固定显存并保持一段时间 |
| GPU Burst | `gpu_burst.py` | 1G | normal | 周期性申请/释放显存 |
| GPU Sleep | `gpu_sleep.py` | 4G | low | 申请显存后长时间空转 |
| OOM 测试 | `gpu_oom.py` | 2G | normal | 模拟 OOM 错误（测试自愈） |
| 小任务 | `gpu_alloc.py` | 0.5G | normal | 小显存任务（测试回填） |
| 简单测试 | `echo` | 1G | high | 简单的 echo 命令 |

---

## 🚀 快速开始

### 第一次使用（推荐）

```bash
# 1. 进入目录
cd /home/cwx/workspace/gpudirector

# 2. 激活环境
conda activate graphAR

# 3. 运行完整版脚本
bash run_examples_with_labgpu.sh

# 或 Python 版本
python run_examples.py
```

### 已经熟悉了，快速测试

```bash
cd /home/cwx/workspace/gpudirector
conda activate graphAR
bash run_examples_simple.sh
```

### 需要监控任务

```bash
cd /home/cwx/workspace/gpudirector
conda activate graphAR
bash run_and_monitor.sh
```

---

## 📝 脚本执行流程

所有脚本的基本流程：

```
1. 启动 Lab-GPU Master 服务
   ↓
2. 注册 GPU 节点
   ↓
3. 提交示例任务
   ↓
4. 执行调度 (lab-gpu server tick)
   ↓
5. 查看任务状态
   ↓
6. (可选) 启动 TUI 界面
```

---

## 💡 执行后可以做什么

### 查看任务状态
```bash
lab-gpu status          # 简要状态
lab-gpu status --json   # 详细状态
```

### 启动 TUI 可视化界面
```bash
lab-gpu tui
```

**TUI 快捷键：**
- `k` - 杀死选中的任务
- `r` - 重试选中的任务
- `t` - 提升任务到队首
- `q` - 退出

### 查看任务日志
```bash
lab-gpu logs 1          # 查看任务 1 的日志
lab-gpu logs 1 -f       # 实时跟踪任务 1 的日志
```

### 手动提交更多任务
```bash
lab-gpu submit --mem 10G --priority normal "python your_script.py"
```

### 执行调度
```bash
lab-gpu server tick
```

---

## 🔧 自定义修改

### 修改 GPU 节点配置

编辑脚本中的这部分：

```bash
lab-gpu server add-node --name node-1 --gpus 2 --vram 24 --gpu-type "RTX 3090"
```

参数说明：
- `--name`: 节点名称
- `--gpus`: GPU 数量
- `--vram`: 每个 GPU 的显存大小（GB）
- `--gpu-type`: GPU 型号

### 添加自己的任务

在脚本中添加：

```bash
lab-gpu submit --mem <显存> --priority <优先级> "你的命令"
```

示例：
```bash
lab-gpu submit --mem 16G --priority high "python train_model.py --epochs 100"
```

---

## ❓ 常见问题

### Q: 脚本执行失败？
**A:** 确保已激活 graphAR 环境：
```bash
conda activate graphAR
```

### Q: 任务提交后看不到？
**A:** 需要执行调度：
```bash
lab-gpu server tick
```

### Q: 如何停止所有任务？
**A:** 使用 TUI 界面（`lab-gpu tui`），按 `k` 杀死任务

### Q: 示例程序在哪里？
**A:** 在 `examples/` 目录：
- `gpu_alloc.py` - GPU 显存分配
- `gpu_burst.py` - 周期性显存申请
- `gpu_sleep.py` - 长时间占用
- `gpu_oom.py` - OOM 模拟

### Q: Mock 模式是什么？
**A:** `--mock` 参数让示例程序在没有真实 GPU 的情况下也能运行，用于测试调度逻辑

---

## 📚 更多信息

- 完整文档：`README.md`
- 中文教程：`使用指南.md`
- 快速参考：`快速参考.txt`
- 英文指南：`quick_start.md`

---

## 🎓 推荐使用流程

1. **第一次使用：** 运行 `python run_examples.py`，了解完整流程
2. **日常使用：** 运行 `bash run_and_monitor.sh`，直接进入 TUI 界面
3. **快速测试：** 运行 `bash run_examples_simple.sh`

---

祝你使用愉快！🚀
