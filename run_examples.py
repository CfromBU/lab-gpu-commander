#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
Lab-GPU ç¤ºä¾‹ä»»åŠ¡æäº¤è„šæœ¬ (Python ç‰ˆæœ¬)
ä½¿ç”¨ examples ç›®å½•ä¸­çš„ç¤ºä¾‹ç¨‹åºï¼Œæ— éœ€é¢„è®¾ tasks.json
"""

import subprocess
import sys
import time
import json
from typing import List, Dict

class LabGPUManager:
    """Lab-GPU ä»»åŠ¡ç®¡ç†å™¨"""
    
    def __init__(self, host="127.0.0.1"):
        self.host = host
        self.task_ids = []
    
    def run_command(self, cmd: List[str], capture=False):
        """æ‰§è¡Œå‘½ä»¤"""
        try:
            if capture:
                result = subprocess.run(cmd, capture_output=True, text=True, check=True)
                return result.stdout.strip()
            else:
                subprocess.run(cmd, check=True)
                return None
        except subprocess.CalledProcessError as e:
            print(f"âŒ å‘½ä»¤æ‰§è¡Œå¤±è´¥: {' '.join(cmd)}")
            print(f"   é”™è¯¯: {e}")
            sys.exit(1)
    
    def start_server(self):
        """å¯åŠ¨ Master æœåŠ¡"""
        print("âš™ï¸  å¯åŠ¨ Master æœåŠ¡...")
        self.run_command(["lab-gpu", "server", "start", "--role", "master", "--host", self.host])
        print("âœ… Master æœåŠ¡å·²å¯åŠ¨")
    
    def add_node(self, name: str, gpus: int, vram: int, gpu_type: str = None):
        """æ·»åŠ  GPU èŠ‚ç‚¹"""
        cmd = ["lab-gpu", "server", "add-node", "--name", name, "--gpus", str(gpus), "--vram", str(vram)]
        if gpu_type:
            cmd.extend(["--gpu-type", gpu_type])
        self.run_command(cmd)
        print(f"âœ… å·²æ·»åŠ èŠ‚ç‚¹: {name} ({gpus}x GPU, {vram}GB)")
    
    def submit_task(self, cmd: str, mem: str, priority: str = "normal", description: str = None):
        """æäº¤ä»»åŠ¡"""
        submit_cmd = ["lab-gpu", "submit", "--mem", mem, "--priority", priority, cmd]
        output = self.run_command(submit_cmd, capture=True)
        
        # æå–ä»»åŠ¡ ID
        if "task" in output.lower():
            task_id = output.split()[-1]
            self.task_ids.append(task_id)
            desc = f" ({description})" if description else ""
            print(f"âœ… å·²æäº¤ä»»åŠ¡ ID: {task_id}{desc}")
            return task_id
        return None
    
    def tick(self):
        """æ‰§è¡Œè°ƒåº¦"""
        print("\nâš™ï¸  æ‰§è¡Œè°ƒåº¦...")
        self.run_command(["lab-gpu", "server", "tick"])
        print("âœ… è°ƒåº¦å®Œæˆ")
    
    def status(self, json_output=False):
        """æŸ¥çœ‹çŠ¶æ€"""
        if json_output:
            output = self.run_command(["lab-gpu", "status", "--json"], capture=True)
            return json.loads(output)
        else:
            self.run_command(["lab-gpu", "status"])
    
    def launch_tui(self):
        """å¯åŠ¨ TUI ç•Œé¢"""
        print("\nğŸ¨ å¯åŠ¨ TUI å¯è§†åŒ–ç•Œé¢...")
        print("\nğŸ’¡ TUI å¿«æ·é”®ï¼š")
        print("   k - æ€æ­»ä»»åŠ¡")
        print("   r - é‡è¯•ä»»åŠ¡")
        print("   t - æå‡åˆ°é˜Ÿé¦–")
        print("   q - é€€å‡º")
        print("â”" * 60)
        time.sleep(2)
        subprocess.run(["lab-gpu", "tui"])


def main():
    print("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
    print("â•‘     Lab-GPU ç¤ºä¾‹ä»»åŠ¡è¿è¡Œè„šæœ¬ (Python ç‰ˆ)                     â•‘")
    print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
    print()
    
    manager = LabGPUManager()
    
    # 1. å¯åŠ¨æœåŠ¡
    print("â”" * 60)
    print("ğŸ“‹ æ­¥éª¤ 1: å¯åŠ¨æœåŠ¡")
    print("â”" * 60)
    manager.start_server()
    print()
    
    # 2. æ·»åŠ èŠ‚ç‚¹
    print("â”" * 60)
    print("ğŸ“‹ æ­¥éª¤ 2: æ·»åŠ  GPU èŠ‚ç‚¹")
    print("â”" * 60)
    manager.add_node("node-1", gpus=2, vram=24, gpu_type="RTX 3090")
    manager.add_node("node-2", gpus=2, vram=48, gpu_type="A100")
    print()
    
    # 3. æŸ¥çœ‹åˆå§‹çŠ¶æ€
    print("â”" * 60)
    print("ğŸ“‹ æ­¥éª¤ 3: åˆå§‹çŠ¶æ€")
    print("â”" * 60)
    manager.status()
    print()
    
    # 4. æäº¤ç¤ºä¾‹ä»»åŠ¡
    print("â”" * 60)
    print("ğŸ“‹ æ­¥éª¤ 4: æäº¤ç¤ºä¾‹ä»»åŠ¡")
    print("â”" * 60)
    print()
    
    # å®šä¹‰ä»»åŠ¡åˆ—è¡¨
    tasks = [
        {
            "cmd": "python examples/gpu_alloc.py --mock --gb 2 --sleep 20",
            "mem": "2G",
            "priority": "high",
            "desc": "GPU æ˜¾å­˜åˆ†é… 2GB"
        },
        {
            "cmd": "python examples/gpu_burst.py --mock --gb 1 --cycles 3",
            "mem": "1G",
            "priority": "normal",
            "desc": "GPU Burst å‘¨æœŸæ€§ç”³è¯·"
        },
        {
            "cmd": "python examples/gpu_sleep.py --mock --gb 4 --sleep 60",
            "mem": "4G",
            "priority": "low",
            "desc": "GPU é•¿æ—¶é—´å ç”¨ 4GB"
        },
        {
            "cmd": "python examples/gpu_oom.py --mock-oom",
            "mem": "2G",
            "priority": "normal",
            "desc": "OOM æµ‹è¯•ï¼ˆè‡ªæ„ˆåŠŸèƒ½ï¼‰"
        },
        {
            "cmd": "python examples/gpu_alloc.py --mock --gb 0.5 --sleep 5",
            "mem": "0.5G",
            "priority": "normal",
            "desc": "å°ä»»åŠ¡ï¼ˆå›å¡«æµ‹è¯•ï¼‰"
        },
        {
            "cmd": "echo 'Lab-GPU Test' && sleep 3",
            "mem": "1G",
            "priority": "high",
            "desc": "ç®€å•æµ‹è¯•ä»»åŠ¡"
        },
    ]
    
    print("ğŸš€ æäº¤ä»»åŠ¡:")
    for i, task in enumerate(tasks, 1):
        print(f"\n   [{i}] {task['desc']}")
        manager.submit_task(
            cmd=task["cmd"],
            mem=task["mem"],
            priority=task["priority"],
            description=f"{task['priority']}, {task['mem']}"
        )
    
    print()
    
    # 5. æ‰§è¡Œè°ƒåº¦
    print("â”" * 60)
    print("ğŸ“‹ æ­¥éª¤ 5: æ‰§è¡Œè°ƒåº¦")
    print("â”" * 60)
    manager.tick()
    print()
    
    # 6. æŸ¥çœ‹ä»»åŠ¡çŠ¶æ€
    print("â”" * 60)
    print("ğŸ“‹ æ­¥éª¤ 6: ä»»åŠ¡çŠ¶æ€")
    print("â”" * 60)
    manager.status()
    print()
    
    # 7. è¯¦ç»†çŠ¶æ€
    print("â”" * 60)
    print("ğŸ“‹ æ­¥éª¤ 7: è¯¦ç»†çŠ¶æ€ (JSON)")
    print("â”" * 60)
    status = manager.status(json_output=True)
    import json
    print(json.dumps(status, indent=2))
    print()
    
    # 8. æ€»ç»“
    print("â•”â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•—")
    print("â•‘     è„šæœ¬æ‰§è¡Œå®Œæˆï¼                                            â•‘")
    print("â•šâ•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•â•")
    print()
    print("ğŸ’¡ æ¥ä¸‹æ¥ä½ å¯ä»¥ï¼š")
    print()
    print("   1ï¸âƒ£  å¯åŠ¨ TUI å¯è§†åŒ–ç•Œé¢ï¼ˆæŒ‰å›è½¦å¯åŠ¨ï¼‰")
    print("      lab-gpu tui")
    print()
    print("   2ï¸âƒ£  æŸ¥çœ‹ç‰¹å®šä»»åŠ¡çš„æ—¥å¿—")
    if manager.task_ids:
        print(f"      lab-gpu logs {manager.task_ids[0]}")
        print(f"      lab-gpu logs {manager.task_ids[0]} -f  # å®æ—¶è·Ÿè¸ª")
    print()
    print("   3ï¸âƒ£  æŸ¥çœ‹ä»»åŠ¡çŠ¶æ€")
    print("      lab-gpu status")
    print()
    print("â”" * 60)
    print(f"ğŸ“ å·²æäº¤ {len(manager.task_ids)} ä¸ªä»»åŠ¡")
    print("â”" * 60)
    print()
    
    # è¯¢é—®æ˜¯å¦å¯åŠ¨ TUI
    try:
        response = input("æ˜¯å¦å¯åŠ¨ TUI ç•Œé¢ï¼Ÿ(y/nï¼Œé»˜è®¤ y): ").strip().lower()
        if response in ['', 'y', 'yes']:
            manager.launch_tui()
    except KeyboardInterrupt:
        print("\n\nğŸ‘‹ å†è§ï¼")
        sys.exit(0)


if __name__ == "__main__":
    main()
