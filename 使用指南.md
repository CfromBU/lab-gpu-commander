# Lab-GPU ä½¿ç”¨æŒ‡å—ï¼ˆä¸­æ–‡ç‰ˆï¼‰

## ğŸ¯ è¿™æ˜¯ä»€ä¹ˆï¼Ÿ

Lab-GPU æ˜¯ä¸€ä¸ª**å®éªŒå®¤ GPU ä»»åŠ¡è°ƒåº¦å™¨**ï¼Œå¯ä»¥å¸®ä½ ï¼š
- ç®¡ç†å¤šäººå…±äº«çš„ GPU æœåŠ¡å™¨
- è‡ªåŠ¨åˆ†é… GPU èµ„æº
- é˜Ÿåˆ—ç®¡ç†ï¼ˆæ”¯æŒä¼˜å…ˆçº§ï¼‰
- è‡ªåŠ¨å¤„ç† OOM é”™è¯¯
- å¯è§†åŒ–ç•Œé¢æŸ¥çœ‹ä»»åŠ¡çŠ¶æ€

## ğŸš€ ä¸‰æ­¥å¿«é€Ÿå¼€å§‹

### ç¬¬ä¸€æ­¥ï¼šå¯åŠ¨æœåŠ¡
```bash
conda activate graphAR
lab-gpu server start --role master --host 127.0.0.1
lab-gpu server add-node --name node-1 --gpus 2 --vram 24
```

### ç¬¬äºŒæ­¥ï¼šæäº¤ä»»åŠ¡
```bash
# æ–¹å¼ 1: å•ä¸ªä»»åŠ¡
lab-gpu submit --mem 10G --priority normal "python train.py"

# æ–¹å¼ 2: æ‰¹é‡ä»»åŠ¡ï¼ˆä½¿ç”¨ä½ çš„ tasks.jsonï¼‰
lab-gpu submit-batch --file tasks.json
```

### ç¬¬ä¸‰æ­¥ï¼šæ‰§è¡Œè°ƒåº¦å¹¶æŸ¥çœ‹
```bash
lab-gpu server tick      # è§¦å‘è°ƒåº¦
lab-gpu status           # æŸ¥çœ‹çŠ¶æ€
lab-gpu tui              # å¯è§†åŒ–ç•Œé¢ï¼ˆæ¨èï¼‰
```

## ğŸ“ tasks.json é…ç½®è¯´æ˜

ä½ çš„ `tasks.json` æ–‡ä»¶æ ¼å¼ï¼š

```json
{
  "tasks": [
    {
      "cmd": "python train_a.py",        // è¦æ‰§è¡Œçš„å‘½ä»¤
      "mem": "10G",                       // éœ€è¦çš„æ˜¾å­˜
      "priority": "normal"                // ä¼˜å…ˆçº§: high/normal/low
    },
    {
      "cmd": "python train_b.py",
      "min_vram_gb": 8,                   // ä¹Ÿå¯ä»¥ç”¨æ•°å­—æŒ‡å®šæ˜¾å­˜ï¼ˆGBï¼‰
      "priority": "high",
      "time_limit": 1200                  // æ—¶é—´é™åˆ¶ï¼ˆç§’ï¼‰
    }
  ]
}
```

**å¯ç”¨å­—æ®µï¼š**
- `cmd` - å¿…å¡«ï¼Œè¦è¿è¡Œçš„å‘½ä»¤
- `mem` - æ˜¾å­˜éœ€æ±‚ï¼Œå¦‚ "10G", "16G"
- `min_vram_gb` - æ˜¾å­˜éœ€æ±‚ï¼ˆæ•°å­—ï¼‰ï¼Œä¸ mem äºŒé€‰ä¸€
- `priority` - ä¼˜å…ˆçº§ï¼š"high"ï¼ˆé«˜ï¼‰, "normal"ï¼ˆæ™®é€šï¼‰, "low"ï¼ˆä½ï¼‰
- `env` - conda ç¯å¢ƒå
- `gpu_type` - GPU å‹å·ï¼Œå¦‚ "RTX 3090"
- `time_limit` - æ—¶é—´é™åˆ¶ï¼ˆç§’ï¼‰

## ğŸ”§ å¸¸ç”¨å‘½ä»¤é€ŸæŸ¥

### æœåŠ¡ç®¡ç†
```bash
# å¯åŠ¨æœåŠ¡
lab-gpu server start --role master --host 127.0.0.1

# æ·»åŠ  GPU èŠ‚ç‚¹
lab-gpu server add-node --name node-1 --gpus 2 --vram 24

# è§¦å‘è°ƒåº¦
lab-gpu server tick

# æŠ¢å ä»»åŠ¡
lab-gpu server preempt --task-id 1
```

### ä»»åŠ¡æäº¤
```bash
# å•ä¸ªä»»åŠ¡
lab-gpu submit --mem 10G --priority normal "python train.py"

# æŒ‡å®šç¯å¢ƒ
lab-gpu submit --mem 10G --env graphAR "python train.py"

# æ‰¹é‡ä»»åŠ¡
lab-gpu submit-batch --file tasks.json

# æµ‹è¯•ï¼ˆä¸å®é™…æäº¤ï¼‰
lab-gpu submit --mem 10G --dry-run "python train.py"
lab-gpu submit-batch --file tasks.json --dry-run
```

### æŸ¥çœ‹çŠ¶æ€
```bash
# ç®€è¦çŠ¶æ€
lab-gpu status

# è¯¦ç»† JSON æ ¼å¼
lab-gpu status --json

# æŸ¥çœ‹ä»»åŠ¡æ—¥å¿—
lab-gpu logs 1          # æŸ¥çœ‹ä»»åŠ¡ 1 çš„æ—¥å¿—
lab-gpu logs 1 -f       # å®æ—¶è·Ÿè¸ªæ—¥å¿—

# TUI å¯è§†åŒ–ç•Œé¢ï¼ˆæ¨èï¼ï¼‰
lab-gpu tui
```

## ğŸ¨ TUI ç•Œé¢ä½¿ç”¨

å¯åŠ¨ TUIï¼š
```bash
lab-gpu tui
```

**å¿«æ·é”®ï¼š**
- `k` - æ€æ­»é€‰ä¸­çš„ä»»åŠ¡
- `r` - é‡è¯•é€‰ä¸­çš„ä»»åŠ¡  
- `t` - æå‡ä»»åŠ¡åˆ°é˜Ÿé¦–
- `q` - é€€å‡º

**å‘½ä»¤ï¼š**
- è¾“å…¥ `list` - åˆ‡æ¢åˆ—è¡¨æ¨¡å¼
- è¾“å…¥ `top 5` - æå‡ä»»åŠ¡ 5 åˆ°é˜Ÿé¦–

## ğŸ’¡ ä½¿ç”¨æŠ€å·§

### 1. ä¼˜å…ˆçº§è®¾ç½®
- **high** - ç´§æ€¥ä»»åŠ¡ï¼Œç«‹å³è°ƒåº¦
- **normal** - æ™®é€šä»»åŠ¡ï¼ŒæŒ‰é¡ºåºæ‰§è¡Œ
- **low** - ä¸æ€¥çš„ä»»åŠ¡ï¼Œå¤œé—´ä¼šè‡ªåŠ¨åŠ é€Ÿ

### 2. æ‰¹é‡æäº¤ä»»åŠ¡
ç¼–è¾‘ä½ çš„ `tasks.json`ï¼Œç„¶åï¼š
```bash
# å…ˆæµ‹è¯•é…ç½®æ˜¯å¦æ­£ç¡®
lab-gpu submit-batch --file tasks.json --dry-run

# ç¡®è®¤åæ‰¹é‡æäº¤
lab-gpu submit-batch --file tasks.json

# è§¦å‘è°ƒåº¦
lab-gpu server tick

# æŸ¥çœ‹çŠ¶æ€
lab-gpu status
```

### 3. æŸ¥çœ‹ä»»åŠ¡è¿›åº¦
```bash
# æ–¹å¼ 1: ä½¿ç”¨ TUIï¼ˆæœ€ç›´è§‚ï¼‰
lab-gpu tui

# æ–¹å¼ 2: æŸ¥çœ‹æ—¥å¿—
lab-gpu logs 1 -f

# æ–¹å¼ 3: JSON çŠ¶æ€
lab-gpu status --json | python -m json.tool
```

### 4. æµ‹è¯•é…ç½®
æäº¤ä»»åŠ¡å‰ï¼Œå…ˆç”¨ `--dry-run` æµ‹è¯•ï¼š
```bash
lab-gpu submit --mem 32G --dry-run "python large_model.py"
```
å¦‚æœè¿”å› `placement: null`ï¼Œè¯´æ˜æ˜¾å­˜ä¸å¤Ÿï¼Œéœ€è¦è°ƒæ•´ã€‚

## ğŸ“‹ å®Œæ•´ç¤ºä¾‹

æˆ‘ä¸ºä½ å‡†å¤‡äº†ä¸€ä¸ªå®Œæ•´çš„ç¤ºä¾‹è„šæœ¬ï¼š

```bash
# è¿è¡Œå®Œæ•´ç¤ºä¾‹
bash example_workflow.sh
```

è¿™ä¸ªè„šæœ¬ä¼šï¼š
1. å¯åŠ¨æœåŠ¡
2. æ·»åŠ  GPU èŠ‚ç‚¹
3. æäº¤ä½ çš„ tasks.json ä¸­çš„ä»»åŠ¡
4. æ‰§è¡Œè°ƒåº¦
5. æ˜¾ç¤ºçŠ¶æ€

## â“ å¸¸è§é—®é¢˜

### Q: ä»»åŠ¡æäº¤åçœ‹ä¸åˆ°ï¼Ÿ
A: éœ€è¦æ‰§è¡Œ `lab-gpu server tick` è§¦å‘è°ƒåº¦

### Q: å¦‚ä½•ä¿®æ”¹ tasks.jsonï¼Ÿ
A: ç›´æ¥ç¼–è¾‘æ–‡ä»¶ï¼Œæ·»åŠ æˆ–ä¿®æ”¹ä»»åŠ¡é…ç½®å³å¯

### Q: å¦‚ä½•æŒ‡å®š conda ç¯å¢ƒï¼Ÿ
A: åœ¨ tasks.json ä¸­æ·»åŠ  `"env": "graphAR"` å­—æ®µ

### Q: æ—¥å¿—æ–‡ä»¶åœ¨å“ªé‡Œï¼Ÿ
A: é»˜è®¤åœ¨ `/nas/logs/{task_id}.log`ï¼Œå¯ä»¥ç”¨ `lab-gpu logs <id>` æŸ¥çœ‹

### Q: å¦‚ä½•å¼ºåˆ¶åœæ­¢ä»»åŠ¡ï¼Ÿ
A: ä½¿ç”¨ `lab-gpu server preempt --task-id <id>`

## ğŸ“ ä¸‹ä¸€æ­¥

1. **ä¿®æ”¹ tasks.json** - æ·»åŠ ä½ çš„å®é™…è®­ç»ƒä»»åŠ¡
2. **è¿è¡Œç¤ºä¾‹** - `bash example_workflow.sh`
3. **ä½¿ç”¨ TUI** - `lab-gpu tui` æŸ¥çœ‹å®æ—¶çŠ¶æ€
4. **æŸ¥çœ‹å®Œæ•´æ–‡æ¡£** - å‚è€ƒ `README.md` å’Œ `quick_start.md`

---

å¦‚æœ‰é—®é¢˜ï¼Œè¯·æŸ¥çœ‹ README.md æˆ–è¿è¡Œ `lab-gpu --help`
